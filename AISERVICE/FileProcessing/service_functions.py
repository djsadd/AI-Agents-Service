'''extract_text(document_file) ‚Äì –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF, DOCX, TXT.

split_text(text) ‚Äì —Ä–∞–∑–±–∏—Ç—å –Ω–∞ —á–∞–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ 500 —Ç–æ–∫–µ–Ω–æ–≤).

get_embedding(chunk_text) ‚Äì –≤—ã–∑–≤–∞—Ç—å OpenAI –∏–ª–∏ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞.

store_in_quadrant(project_id, chunk_id, vector) ‚Äì —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Quadrant (–∏–ª–∏ –ª—é–±—É—é –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î).

'''

import uuid
import requests

from sentence_transformers import SentenceTransformer
from typing import List
import fitz  # PyMuPDF
from docx import Document
import os


def extract_text(file_path):
    _, ext = os.path.splitext(file_path)

    if ext.lower() == ".pdf":
        return extract_text_from_pdf(file_path)
    elif ext.lower() == ".docx":
        return extract_text_from_docx(file_path)
    elif ext.lower() == ".txt":
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    else:
        raise ValueError(f"Unsupported file type: {ext}")


def extract_text_from_pdf(file_path):
    text = ""
    with fitz.open(file_path) as doc:
        for page in doc:
            text += page.get_text()
    return text


def extract_text_from_docx(file_path):
    doc = Document(file_path)
    return "\n".join([p.text for p in doc.paragraphs])

import re

def split_text(text, chunk_size=500, overlap=50):
    # –†–∞–∑–¥–µ–ª–∏–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –Ω–∞ —Å–ª–æ–≤–∞ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ç–æ–∫–µ–Ω—ã –ø–æ–∑–∂–µ)
    words = text.split()
    chunks = []

    for i in range(0, len(words), chunk_size - overlap):
        chunk = words[i:i + chunk_size]
        chunks.append(" ".join(chunk))

    return chunks


embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')

def get_embeddings(text_chunks: List[str]) -> List[List[float]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (—á–∞–Ω–∫–æ–≤).
    """
    embeddings = embedding_model.encode(
        text_chunks,
        normalize_embeddings=True,
        show_progress_bar=True
    )
    return embeddings


QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "my_collection"


QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "my_collection"
VECTOR_SIZE = 768  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –ø–æ–º–µ–Ω—è–π –µ—Å–ª–∏ —É —Ç–µ–±—è –¥—Ä—É–≥–∞—è

def ensure_qdrant_collection():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant –∏ —Å–æ–∑–¥–∞—ë—Ç –µ—ë –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    url = f"{QDRANT_URL}/collections/{COLLECTION_NAME}"

    # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è
    resp = requests.get(url)
    if resp.status_code == 200:
        print("‚ÑπÔ∏è –ö–æ–ª–ª–µ–∫—Ü–∏—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        return
    elif resp.status_code != 404:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {resp.text}")
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    payload = {
        "vectors": {
            "size": VECTOR_SIZE,
            "distance": "Cosine"
        }
    }
    create_resp = requests.put(url, json=payload)
    if create_resp.status_code == 200:
        print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω–∞.")
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {create_resp.text}")


def upload_to_qdrant(embeddings, texts, file_id):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–∞–Ω–∫–∏ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –≤ Qdrant.
    """
    ensure_qdrant_collection()  # ‚Üê –¥–æ–±–∞–≤–∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏

    url = f"{QDRANT_URL}/collections/{COLLECTION_NAME}/points"

    points = []
    for vector, text in zip(embeddings, texts):
        point = {
            "id": str(uuid.uuid4()),
            "vector": vector.tolist(),  # ‚Üê –¥–æ–±–∞–≤–ª–µ–Ω–æ .tolist()
            "payload": {
                "text": text,
                "file_id": file_id
            }
        }
        points.append(point)

    payload = {
        "points": points
    }

    try:
        response = requests.put(url, json=payload)
        response.raise_for_status()
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(points)} —Ç–æ—á–µ–∫ –≤ Qdrant.")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤ Qdrant: {e}")

TEST_FILE_PATH = r"C:\Users\User\Desktop\AI-Agents-Service\AISERVICE\tests files\test.docx"

def test_pipeline():
    print("üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º pipeline –Ω–∞ —Ñ–∞–π–ª–µ:", TEST_FILE_PATH)

    # 1. –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    text = extract_text(TEST_FILE_PATH)
    assert isinstance(text, str) and len(text) > 0, "‚ùå –¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á—ë–Ω"

    # 2. –†–∞–∑–±–∏–µ–Ω–∏–µ
    chunks = split_text(text)
    assert isinstance(chunks, list) and len(chunks) > 0, "‚ùå –ß–∞–Ω–∫–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω—ã"
    print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

    # 3. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = get_embeddings(chunks)
    assert len(embeddings) == len(chunks), "‚ùå –≠–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ —Å—Ç–æ–ª—å–∫–æ –∂–µ, —Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤"
    assert len(embeddings[0]) == 768, "‚ùå –†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –Ω–µ 768"
    print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã")

    # 4. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant
    upload_to_qdrant(embeddings, chunks, file_id="test_file_for_pipeline")
    print("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Qdrant")

if __name__ == "__main__":
    test_pipeline()
